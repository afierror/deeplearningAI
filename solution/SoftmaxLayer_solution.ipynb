{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2GML1MirEX8"
   },
   "source": [
    "# ðŸ§ª Lab Exercise: Implementing a Softmax Layer for Neural Networks\n",
    "\n",
    "In this lab, you'll dive into the world of neural networks by building a **Softmax layer** from scratch! ðŸŽ¯ This layer is essential for handling **multi-class classification** problems, helping models predict probabilities across multiple categories. Youâ€™ll implement both the **forward pass** (to compute probabilities) and the **backward pass** (to compute gradients) of the softmax operation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Learning Objectives\n",
    "\n",
    "By the end of this lab, youâ€™ll be able to:\n",
    "\n",
    "- ðŸ”¢ **Understand** the mathematical formulation of the softmax function  \n",
    "- ðŸ“ˆ **Implement** the forward pass to generate a probability distribution  \n",
    "- ðŸ” **Implement** the backward pass to compute gradients during backpropagation  \n",
    "- ðŸ› ï¸ **Handle** numerical stability issues to ensure accurate results  \n",
    "\n",
    "---\n",
    "\n",
    "Letâ€™s get started and turn theory into code! ðŸ’»ðŸ”¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l8MbB-LIpR5X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass  # No need for cache if only implementing forward pass\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Compute the softmax of input Z (forward pass only)\n",
    "\n",
    "        Args:\n",
    "            Z: Input array of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns:\n",
    "            A: Probability distribution after softmax, same shape as Z\n",
    "        \"\"\"\n",
    "        # Shift values by max for numerical stability\n",
    "        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute exponentials\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "\n",
    "        # Compute softmax probabilities\n",
    "        A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V5wEcgYRl00"
   },
   "source": [
    "# Assessment Note ðŸ“\n",
    "\n",
    "The **unit test code** provided is designed to automatically evaluate your implementation of the softmax layer. When you run these tests against your completed code, they will verify:\n",
    "\n",
    "## How to Use the Tests ðŸ› \n",
    "1. Complete your `SoftmaxLayer` implementation âœ\n",
    "2. Run the test cell â–¶\n",
    "3. All tests should pass if correct âœ…\n",
    "4. If any fail, check error messages to debug ðŸž\n",
    "\n",
    "\n",
    "**Happy coding!** ðŸš€ Let's make sure your softmax layer works perfectly! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RqZCEbcFRgGz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_probabilities (__main__.TestSoftmaxForward)\n",
      "Test if output probabilities are correct ... ok\n",
      "test_forward_shape (__main__.TestSoftmaxForward)\n",
      "Test if output shape matches input shape ... ok\n",
      "test_forward_stability (__main__.TestSoftmaxForward)\n",
      "Test numerical stability with large inputs ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class TestSoftmaxForward(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.softmax = SoftmaxLayer()\n",
    "        self.tol = 1e-6\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        \"\"\"Test if output shape matches input shape\"\"\"\n",
    "        Z = np.array([[1, 2, 3], [0, 0, 0]])\n",
    "        A = self.softmax.forward(Z)\n",
    "        try:\n",
    "            self.assertEqual(A.shape, Z.shape)\n",
    "        except AssertionError:\n",
    "            print(\"\\nðŸ”´ Error in output shape!\")\n",
    "            print(f\"Expected shape: {Z.shape}, Got: {A.shape}\")\n",
    "            print(\"Make sure your softmax implementation returns an array with the same shape as the input\")\n",
    "            raise\n",
    "\n",
    "    def test_forward_probabilities(self):\n",
    "        \"\"\"Test if output probabilities are correct\"\"\"\n",
    "        Z = np.array([[1, 2, 3]])\n",
    "        A = self.softmax.forward(Z)\n",
    "\n",
    "        # Test row sums\n",
    "        row_sums = np.sum(A, axis=1)\n",
    "        try:\n",
    "            np.testing.assert_allclose(row_sums, [1.0], atol=self.tol)\n",
    "        except AssertionError:\n",
    "            print(\"\\nðŸ”´ Error in probability sums!\")\n",
    "            print(f\"Row sums should be approximately 1.0, Got: {row_sums}\")\n",
    "            print(\"Your softmax probabilities don't sum to 1. Check your normalization step\")\n",
    "            print(\"Remember: A = exp(Z_shifted) / sum(exp(Z_shifted), keeping dimensions with keepdims=True\")\n",
    "            raise\n",
    "\n",
    "        # Test known values\n",
    "        expected = np.array([[0.09003057, 0.24472847, 0.66524096]])\n",
    "        try:\n",
    "            np.testing.assert_allclose(A, expected, atol=self.tol)\n",
    "        except AssertionError:\n",
    "            print(\"\\nðŸ”´ Error in probability values!\")\n",
    "            print(f\"Expected: {expected}\")\n",
    "            print(f\"Got: {A}\")\n",
    "            print(\"Your softmax values don't match expected probabilities\")\n",
    "            print(\"Did you remember to shift values by max(Z) before exponentiation?\")\n",
    "            print(\"The correct steps are:\")\n",
    "            print(\"1. Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\")\n",
    "            print(\"2. exp_Z = np.exp(Z_shifted)\")\n",
    "            print(\"3. A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\")\n",
    "            raise\n",
    "\n",
    "    def test_forward_stability(self):\n",
    "        \"\"\"Test numerical stability with large inputs\"\"\"\n",
    "        Z = np.array([[1000, 1001, 1002]])\n",
    "        A = self.softmax.forward(Z)\n",
    "        try:\n",
    "            np.testing.assert_allclose(np.sum(A, axis=1), [1.0], atol=self.tol)\n",
    "        except AssertionError:\n",
    "            print(\"\\nðŸ”´ Numerical stability problem!\")\n",
    "            print(\"Your softmax fails with large input values\")\n",
    "            print(\"Did you implement the max-shifting trick for numerical stability?\")\n",
    "            print(\"Solution: Subtract np.max(Z) before exponentiation to prevent overflow\")\n",
    "            raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Running tests...\\n\")\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPhaFE2Rns8D"
   },
   "source": [
    "## ðŸ” Instructor Notes: Softmax Implementation Solution\n",
    "\n",
    "### ðŸŽ¯ Key Learning Objectives\n",
    "1. Numerical stability handling in exponential operations\n",
    "2. Proper broadcasting in vectorized operations\n",
    "3. Validation of probability distribution properties\n",
    "\n",
    "### âš ï¸ Common Pitfalls to Address\n",
    "```python\n",
    "A = np.exp(Z) / np.sum(np.exp(Z), axis=1)  # Fails with large Z values\n",
    "```\n",
    "\n",
    "### âš ï¸ Implementation Insights\n",
    "* The `keepdims=True` is crucial for proper broadcasting. Without it, substraction would fail due to shape mismatch:\n",
    "```python\n",
    "np.max(Z, axis=1, keepdims=True)  # Maintains (n,1) shape\n",
    "```\n",
    "\n",
    "### ï¿½ Debugging Tips\n",
    "* If probabilities do not sum to 1:\n",
    "  * Verify interemediate shapes with `Z_shifted.shape`\n",
    "  * Check `np.sum()`includes `keepdims=True`\n",
    "* For NaN values:\n",
    "  * Likely overflow - confirm max-shifting is implemented\n",
    "  * test with extreme values (>>1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
