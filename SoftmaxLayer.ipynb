{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Lab Exercise: Implementing a Softmax Layer for Neural Networks\n",
        "\n",
        "In this lab, you'll dive into the world of neural networks by building a **Softmax layer** from scratch! üéØ This layer is essential for handling **multi-class classification** problems, helping models predict probabilities across multiple categories. You‚Äôll implement the **forward pass** only (to compute probabilities) of the softmax operation.\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Learning Objectives\n",
        "\n",
        "By the end of this lab, you‚Äôll be able to:\n",
        "\n",
        "- üî¢ **Understand** the mathematical formulation of the softmax function  \n",
        "- üìà **Implement** the forward pass to generate a probability distribution  \n",
        "- üõ†Ô∏è **Handle** numerical stability issues to ensure accurate results  \n",
        "\n",
        "---\n",
        "\n",
        "Let‚Äôs get started and turn theory into code! üíªüî•\n",
        "\n",
        "## üìù **Instructions**\n",
        "1. **Starter Code Provided**: You'll find a partially completed SoftmaxLayer class below.\n",
        "2. **Your Code**: Implement Softmax Layer using the following three steps:\n",
        "  1. Shift values by max for numerical stability\n",
        "  2. Compute exponentials\n",
        "  3. Compute softmax probabilities\n",
        "2. **Where to Code:**\n",
        "    - Only modify sections marked with:\n",
        "    ```python\n",
        "     # YOUR CODE GOES HERE\n",
        "     ```\n",
        "    - Do not change any other parts of the code\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FEtLGzOoleag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment Note üìù\n",
        "\n",
        "The **unit test code** provided is designed to automatically evaluate your implementation of the softmax layer. When you run these tests against your completed code, they will verify:\n",
        "\n",
        "## How to Use the Tests üõ†\n",
        "1. Complete your `SoftmaxLayer` implementation ‚úç\n",
        "2. Run the test cell ‚ñ∂\n",
        "3. All tests should pass if correct ‚úÖ\n",
        "4. If any fail, check error messages to debug üêû\n",
        "\n",
        "\n",
        "**Happy coding!** üöÄ Let's make sure your softmax layer works perfectly! üéâ"
      ],
      "metadata": {
        "id": "-SRBUBwKlfxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wOInBuuakNIr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self):\n",
        "        pass  # No need for cache if only implementing forward pass\n",
        "\n",
        "    def forward(self, Z):\n",
        "        \"\"\"Compute the softmax of input Z (forward pass only)\n",
        "\n",
        "        Args:\n",
        "            Z: Input array of shape (batch_size, num_classes)\n",
        "\n",
        "        Returns:\n",
        "            A: Probability distribution after softmax, same shape as Z\n",
        "        \"\"\"\n",
        "        # Shift values by max for numerical stability\n",
        "        Z_shifted = None #YOUR CODE GOES HERE\n",
        "\n",
        "        # Compute exponentials\n",
        "        exp_Z = None #YOUR CODE GOES HERE\n",
        "\n",
        "        # Compute softmax probabilities\n",
        "        A = None #YOUR CODE GOES HERE\n",
        "\n",
        "        return A"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "class TestSoftmaxForward(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.softmax = SoftmaxLayer()\n",
        "        self.tol = 1e-6\n",
        "\n",
        "    def test_forward_shape(self):\n",
        "        \"\"\"Test if output shape matches input shape\"\"\"\n",
        "        Z = np.array([[1, 2, 3], [0, 0, 0]])\n",
        "        A = self.softmax.forward(Z)\n",
        "        try:\n",
        "            self.assertEqual(A.shape, Z.shape)\n",
        "        except AssertionError:\n",
        "            print(\"\\nüî¥ Error in output shape!\")\n",
        "            print(f\"Expected shape: {Z.shape}, Got: {A.shape}\")\n",
        "            print(\"Make sure your softmax implementation returns an array with the same shape as the input\")\n",
        "            raise\n",
        "\n",
        "    def test_forward_probabilities(self):\n",
        "        \"\"\"Test if output probabilities are correct\"\"\"\n",
        "        Z = np.array([[1, 2, 3]])\n",
        "        A = self.softmax.forward(Z)\n",
        "\n",
        "        # Test row sums\n",
        "        row_sums = np.sum(A, axis=1)\n",
        "        try:\n",
        "            np.testing.assert_allclose(row_sums, [1.0], atol=self.tol)\n",
        "        except AssertionError:\n",
        "            print(\"\\nüî¥ Error in probability sums!\")\n",
        "            print(f\"Row sums should be approximately 1.0, Got: {row_sums}\")\n",
        "            print(\"Your softmax probabilities don't sum to 1. Check your normalization step\")\n",
        "            print(\"Remember: A = exp(Z_shifted) / sum(exp(Z_shifted), keeping dimensions with keepdims=True\")\n",
        "            raise\n",
        "\n",
        "        # Test known values\n",
        "        expected = np.array([[0.09003057, 0.24472847, 0.66524096]])\n",
        "        try:\n",
        "            np.testing.assert_allclose(A, expected, atol=self.tol)\n",
        "        except AssertionError:\n",
        "            print(\"\\nüî¥ Error in probability values!\")\n",
        "            print(f\"Expected: {expected}\")\n",
        "            print(f\"Got: {A}\")\n",
        "            print(\"Your softmax values don't match expected probabilities\")\n",
        "            print(\"Did you remember to shift values by max(Z) before exponentiation?\")\n",
        "            print(\"The correct steps are:\")\n",
        "            print(\"1. Z_shifted = Z - np.max(Z, axis=1, keepdims=True)\")\n",
        "            print(\"2. exp_Z = np.exp(Z_shifted)\")\n",
        "            print(\"3. A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\")\n",
        "            raise\n",
        "\n",
        "    def test_forward_stability(self):\n",
        "        \"\"\"Test numerical stability with large inputs\"\"\"\n",
        "        Z = np.array([[1000, 1001, 1002]])\n",
        "        A = self.softmax.forward(Z)\n",
        "        try:\n",
        "            np.testing.assert_allclose(np.sum(A, axis=1), [1.0], atol=self.tol)\n",
        "        except AssertionError:\n",
        "            print(\"\\nüî¥ Numerical stability problem!\")\n",
        "            print(\"Your softmax fails with large input values\")\n",
        "            print(\"Did you implement the max-shifting trick for numerical stability?\")\n",
        "            print(\"Solution: Subtract np.max(Z) before exponentiation to prevent overflow\")\n",
        "            raise\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Running tests...\\n\")\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2)"
      ],
      "metadata": {
        "id": "2jDLw7CpkT8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5IuOnjvke7M"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}